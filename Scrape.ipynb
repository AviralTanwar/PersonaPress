{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium 4:\n",
    "from selenium import webdriver\n",
    "\n",
    "# Starting/Stopping Driver: can specify ports or location but not remote access\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "# Manages Binaries needed for WebDriver without installing anything directly\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Allows searchs similar to beautiful soup: find_all\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Try to establish wait times for the page to load\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Wait for specific condition based on defined task: web elements, boolean are examples\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Used for keyboard movements, up/down, left/right,delete, etc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Locate elements on page and throw error if they do not exist\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# In general to use with timing our function calls to Indeed\n",
    "import time\n",
    "\n",
    "# Assist with creating incremental timing for our scraping to seem more human\n",
    "from time import sleep\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests \n",
    "import random\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint\n",
    "\n",
    "# For webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing and creating xml data\n",
    "# from lxml import etree as et\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "\n",
    "# Multi Threading\n",
    "import threading\n",
    "\n",
    "# Threading:\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "import chromedriver_autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request #1\n",
      "User-Agent Sent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "User-Agent Received: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "\n",
      "Request #2\n",
      "User-Agent Sent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\n",
      "User-Agent Received: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\n",
      "\n",
      "Request #3\n",
      "User-Agent Sent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "User-Agent Received: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "\n",
      "Request #4\n",
      "User-Agent Sent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "User-Agent Received: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\n",
      "\n",
      "Request #5\n",
      "User-Agent Sent: Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\n",
      "User-Agent Received: Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting random UA Agent so as not get detected by the anti-bot system\n",
    "url = 'https://httpbin.org/headers' \n",
    "\n",
    "user_agent_list = [ \n",
    "\t'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36', \n",
    "\t'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36', \n",
    "\t'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36' \n",
    "    ]\n",
    "    # Returns a random user agent from the user_agent_list.\n",
    "    # return random.choice(user_agent_list)\n",
    "\n",
    "for i in range (1,6): \n",
    "\tuser_agent = random.choice(user_agent_list)\n",
    "\t# Checking if the response is getting blocked\n",
    "\theaders = {'User-Agent': user_agent} \n",
    "\tresponse = requests.get(url, headers=headers) \n",
    "\treceived_ua = response.json()['headers']['User-Agent'] \n",
    "\tprint(\"Request #%d\\nUser-Agent Sent: %s\\nUser-Agent Received: %s\\n\" % (i, user_agent, received_ua)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_user_agent():\n",
    "    # Define a custom user agents for ua rotation\n",
    "    user_agent_list = [ \n",
    "\t'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36', \n",
    "\t'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36', \n",
    "\t'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36' \n",
    "    ]\n",
    "    # Returns a random user agent from the user_agent_list.\n",
    "    return random.choice(user_agent_list)\n",
    "\n",
    "def calling_options():\n",
    "    # Allows you to cusotmize: ingonito mode, maximize window size, headless browser, disable certain features, etc\n",
    "    option= webdriver.ChromeOptions()\n",
    "\n",
    "    # Going undercover:\n",
    "    option.add_argument(\"--incognito\")\n",
    "\n",
    "    # Consider this if the application works and you know how it works for speed ups and rendering!\n",
    "\n",
    "    option.add_argument('--headless')\n",
    "\n",
    "    option.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    option.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "\n",
    "    user_agent = get_random_user_agent()\n",
    "    option.add_argument(f\"user-agent={user_agent}\")\n",
    "    return option\n",
    "\n",
    "def get_random_feed(feed):\n",
    "    return random.choice(feed)\n",
    "\n",
    "# chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(selected_feed):\n",
    "    if selected_feed == 'international':\n",
    "        url_feed = ['https://www.theguardian.com/international', 'https://www.bbc.com/news']\n",
    "    elif selected_feed == 'tech':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'national':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'current-affairs':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'entertainment':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://www.pinkvilla.com/']\n",
    "    elif selected_feed == 'fashion':\n",
    "        url_feed = ['https://www.pinkvilla.com/', 'https://timesofindia.indiatimes.com/']\n",
    "    elif selected_feed == 'finance':\n",
    "        url_feed = ['https://www.financialexpress.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'sports':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'politics':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'education':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    elif selected_feed == 'miscellaneous':\n",
    "        url_feed = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/']\n",
    "    return url_feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the URLs\n",
    "for url in urls:\n",
    "    start = time.time()\n",
    "    options = calling_options()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    # Add a sleep to simulate human-like interactions\n",
    "    sleep(randint(2, 6))\n",
    "    # Add your scraping logic here\n",
    "    print(url)\n",
    "    selected_feed = random.choice(feed)\n",
    "\n",
    "    url = 'https://indianexpress.com/'\n",
    "\n",
    "# Function to get the desired URL\n",
    "def get_desired_url(url):\n",
    "    # Set up Selenium WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the navbar to be visible\n",
    "        navbar = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.ID, \"navbar\"))\n",
    "        )\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the 'business' link under the navbar\n",
    "        business_link = navbar.find('a', text='Business')\n",
    "\n",
    "        if business_link:\n",
    "            # Click on the 'business' link if found\n",
    "            business_link.click()\n",
    "\n",
    "            # Wait for the new page to load (you may need to adjust the waiting time)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.url_changes(url)\n",
    "            )\n",
    "\n",
    "            # Print the URL after clicking on 'business'\n",
    "            print(\"URL after clicking on 'Business':\", driver.current_url)\n",
    "        else:\n",
    "            print(\"Business link not found in the navbar.\")\n",
    "\n",
    "    finally:\n",
    "        # Close the WebDriver session\n",
    "        driver.quit()\n",
    "\n",
    "# Main program\n",
    "get_desired_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.ndtv.com/', 'https://timesofindia.indiatimes.com/', 'https://www.thehindu.com/', 'https://www.financialexpress.com/', \n",
    "        'https://www.pinkvilla.com/', 'https://www.theguardian.com/international', 'https://www.bbc.com/news']\n",
    "\n",
    "feeds = [ \n",
    "        'international', 'tech', 'national', \n",
    "        'current-affairs', 'entertainment', 'fashion', \n",
    "        'finance', 'sports', 'politics', \n",
    "        'education', 'miscellaneous' ]\n",
    "\n",
    "# \n",
    "# ENTER THE RANGE OF LENGTH OF THE NEWS ARTCLES\n",
    "# \n",
    "min_limit = 50 \n",
    "max_limit = 60\n",
    "\n",
    "\n",
    "# \n",
    "# ENTER THE FEED USER WANTS\n",
    "# \n",
    "feed = [ \n",
    "        'international', 'tech', 'national', \n",
    "        'current-affairs', 'entertainment', 'fashion', \n",
    "        'finance', 'sports', 'politics', \n",
    "        'education', 'miscellaneous' ]\n",
    "\n",
    "selected_feed = get_random_feed(feed)\n",
    "\n",
    "url_feed = get_url(selected_feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ndtv.com/\n",
      "https://timesofindia.indiatimes.com/\n",
      "https://www.thehindu.com/\n",
      "https://www.financialexpress.com/\n",
      "https://www.pinkvilla.com/\n",
      "https://www.theguardian.com/international\n",
      "https://www.bbc.com/news\n"
     ]
    }
   ],
   "source": [
    "# Loop through the URLs\n",
    "for url in urls:\n",
    "    start = time.time()\n",
    "    options = calling_options()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    # Add a sleep to simulate human-like interactions\n",
    "    sleep(randint(2, 6))\n",
    "    # Add your scraping logic here\n",
    "    print(url)\n",
    "    selected_feed = random.choice(feed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "options = calling_options()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://www.bbc.com/news')\n",
    "# Add a sleep to simulate human-like interactions\n",
    "sleep(random.randint(2, 6))\n",
    "\n",
    "already_read = set()\n",
    "data_list = []\n",
    "\n",
    "while True :\n",
    "    # Find all elements with the specific class 'ssrcss-1amy2cn-ListItem e1gp961v0'\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, '[class^=\"ssrcss-13h7haz-ListItem\"], [class^=\"ssrcss-7uxr49-RichTextContainer\"], [class^=\"ssrcss-11r1m41-RichTextComponentWrapper\"], [class^= \"li.ssrcss-1amy2cn-ListItem.e1gp961v0\"]')\n",
    "\n",
    "    # Extract text and links from each element\n",
    "    for element in elements:\n",
    "        link = element.find_element(By.TAG_NAME, 'a')\n",
    "        link_text = link.text\n",
    "        link_href = link.get_attribute('href')\n",
    "        \n",
    "        # Check if link has already been visited\n",
    "        if link_href not in already_read:\n",
    "            # Click on the link\n",
    "            link.click()\n",
    "            already_read.add(link_href)\n",
    "            print(f\"Visited link: {link_text}\")\n",
    "            # Wait for the page to load\n",
    "            sleep(random.randint(5, 10))\n",
    "            # Find all elements with the specified class on the visited page\n",
    "            article_elements = driver.find_elements(By.CSS_SELECTOR, '.ssrcss-pv1rh6-ArticleWrapper [class*=\"RichTextComponentWrapper\"][data-component=\"text-block\"]')\n",
    "            # Extract text from each article element and append to data list\n",
    "            article_data = [article.text for article in article_elements]\n",
    "            # print(\"Article Data:\", article_data)  # Debugging: Print article data\n",
    "            data_list.append(article_data)\n",
    "            # Go back to the previous page\n",
    "            driver.back()\n",
    "            # Re-find the elements after navigating back\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Link already visited: {link_text}\")\n",
    "\n",
    "    # Break the loop if all links have been visited\n",
    "    if len(already_read) == len(elements):\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print or use the data list as needed\n",
    "# print(\"Data List:\", data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(already_read)\n",
    "print(len(already_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "options = calling_options()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://www.bbc.com/news')\n",
    "# Add a sleep to simulate human-like interactions\n",
    "sleep(random.randint(2, 6))\n",
    "\n",
    "already_read = set()\n",
    "data_list = []\n",
    "\n",
    "while True :\n",
    "    # Find all elements with either class name 'ssrcss-13h7haz-ListItem', 'ssrcss-7uxr49-RichTextContainer', or 'ssrcss-11r1m41-RichTextComponentWrapper'\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, '[class^=\"ssrcss-13h7haz-ListItem\"], [class^=\"ssrcss-7uxr49-RichTextContainer\"], [class^=\"ssrcss-11r1m41-RichTextComponentWrapper\"]')\n",
    "\n",
    "    # Extract text and links from each element\n",
    "    for element in elements:\n",
    "        link = element.find_element(By.TAG_NAME, 'a')\n",
    "        link_text = link.text\n",
    "        link_href = link.get_attribute('href')\n",
    "        \n",
    "        # Check if link has already been visited\n",
    "        if link_href not in already_read:\n",
    "            # Click on the link\n",
    "            link.click()\n",
    "            already_read.add(link_href)\n",
    "            print(f\"Visited link: {link_text}\")\n",
    "            # Wait for the page to load\n",
    "            sleep(random.randint(5, 10))\n",
    "            # Find all elements with the specified class on the visited page\n",
    "            article_elements = driver.find_elements(By.CSS_SELECTOR, '.ssrcss-pv1rh6-ArticleWrapper [class*=\"RichTextComponentWrapper\"][data-component=\"text-block\"]')\n",
    "            # Extract text from each article element and append to data list\n",
    "            article_data = [article.text for article in article_elements]\n",
    "            # print(\"Article Data:\", article_data)  # Debugging: Print article data\n",
    "            data_list.append(article_data)\n",
    "            # Go back to the previous page\n",
    "            driver.back()\n",
    "            # Re-find the elements after navigating back\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Link already visited: {link_text}\")\n",
    "\n",
    "    # Break the loop if all links have been visited\n",
    "    if len(already_read) == len(elements):\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print or use the data list as needed\n",
    "# print(\"Data List:\", data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "start = time.time()\n",
    "options = calling_options()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://www.bbc.com/news')\n",
    "# Add a sleep to simulate human-like interactions\n",
    "sleep(random.randint(2, 6))\n",
    "\n",
    "already_read = set()\n",
    "data_list = []\n",
    "\n",
    "while True:\n",
    "    # Find all elements with either class name 'ssrcss-13h7haz-ListItem', 'ssrcss-7uxr49-RichTextContainer', or 'ssrcss-11r1m41-RichTextComponentWrapper'\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, '[class^=\"ssrcss-13h7haz-ListItem\"], [class^=\"ssrcss-7uxr49-RichTextContainer\"], [class^=\"ssrcss-11r1m41-RichTextComponentWrapper\"]')\n",
    "\n",
    "    # Extract text and links from each element\n",
    "    for element in elements:\n",
    "        link = element.find_element(By.TAG_NAME, 'a')\n",
    "        link_text = link.text\n",
    "        link_href = link.get_attribute('href')\n",
    "        \n",
    "        # Check if link has already been visited\n",
    "        if link_href not in already_read:\n",
    "            # Click on the link\n",
    "            link.click()\n",
    "            already_read.add(link_href)\n",
    "            print(f\"Visited link: {link_text}\")\n",
    "            # Wait for the page to load\n",
    "            sleep(random.randint(5, 10))\n",
    "            # Find all paragraph elements within the specified class on the visited page\n",
    "            article_elements = driver.find_elements(By.CSS_SELECTOR, '.ssrcss-7uxr49-RichTextContainer .ssrcss-1q0x1qg-Paragraph')\n",
    "            \n",
    "            # Debugging: Print the number of article elements found\n",
    "            print(\"Number of article elements found:\", len(article_elements))\n",
    "            \n",
    "            # Extract text from each article element and append to data list\n",
    "            article_data = [article.text for article in article_elements]\n",
    "            print(\"Article Data:\", article_data)  # Debugging: Print article data\n",
    "            data_list.append(article_data)\n",
    "            # Go back to the previous page\n",
    "            driver.back()\n",
    "            # Re-find the elements after navigating back\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Link already visited: {link_text}\")\n",
    "\n",
    "    # Break the loop if all links have been visited\n",
    "    if len(already_read) == len(elements):\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print or use the data list as needed\n",
    "# print(\"Data List:\", data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# options = calling_options()\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# driver.get(random.choice(url_feed))\n",
    "# # Add a sleep to simulate human-like interactions\n",
    "# sleep(randint(2, 6))\n",
    "# # Add your scraping logic here\n",
    "# print(url_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongodb+srv://aviraltanwar:AviralTanwar@cluster0.6xjhtom.mongodb.net/\n",
    "\n",
    "# c:\\\\Users\\\\Aviral Tanwar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\chromedriver_autoinstaller\\\\121\\\\chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
